{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032389,
     "end_time": "2021-01-04T09:44:47.766504",
     "exception": false,
     "start_time": "2021-01-04T09:44:47.734115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 0. Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030902,
     "end_time": "2021-01-04T09:44:47.828652",
     "exception": false,
     "start_time": "2021-01-04T09:44:47.797750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031557,
     "end_time": "2021-01-04T09:44:47.891447",
     "exception": false,
     "start_time": "2021-01-04T09:44:47.859890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* job_level : Level Jabatan Pekerja\n",
    "* job_duration_in_current_job_level : Masa Kerja pada job level saat ini\n",
    "* person_level : Level personal Pekerja\n",
    "* job_duration_in_current_person_level : Masa Kerja pada person level saat ini\n",
    "* job_duration_in_current_branch : Masa Kerja pada unit kerja saat ini\n",
    "* Employee_type : Tipe Pekerja ( 3 tipe Relationship Manager, tipe A : Mantri Kupedes, B : Mantri KUR , dan C: Mantri Briguna)\n",
    "* Employee_status : Status Pekerja (tetap/kontrak)\n",
    "* gender : Jenis Kelamin\n",
    "* age : Tahun Lahir\n",
    "* marital_status_maried(Y/N) : Status Pernikahan (Y / N)\n",
    "* number_of_dependences : Jumlah anak dalam tanggungan\n",
    "* number_of_dependences (male) : Jumlah anak dalam tanggungan khusus laki - laki\n",
    "* number_of_dependences (female) : Jumlah anak dalam tanggungan khusus perempuan\n",
    "* Education_level : Tingkat pendidikan (0: Internal course/sem, 1: SLTA/Setingkat, 2: Diploma 1, 3: * Diploma 3/4, 4: Strata1, 5:Strata2)\n",
    "* GPA : IPK\n",
    "* year_graduated : Tahun lulus\n",
    "* job_duration_as_permanent_worker : lama bekerja sebagai pekerja tetap\n",
    "* job_duration_from_training : lama bekerja mulai dari training\n",
    "* branch_rotation : Jumlah rotasi pindah unit kerja\n",
    "* job_rotation : jumlah rotasi pindah jabatan\n",
    "* assign_of_otherposition : jumlah rotasi penugasan\n",
    "* annual leave : cuti tahunan\n",
    "* sick_leaves : izin sakit\n",
    "* Best Performance : Termasuk dalam best performance (1/0)\n",
    "* Avg_achievement_% : rata - rata presentase pencapaian terhadap target selama setahun\n",
    "* Last_achievement_% : presentase pencapaian triwulan terakhir terhadap target\n",
    "* Achievement_above_100%_during3quartal : Jumlah pencapaian diatas 100% dalam 3 tahun terkahir\n",
    "* achievement_target_1 : status pencapaian target kategori 1\n",
    "* achievement_target_2 : status pencapaian target kategori 2\n",
    "* achievement_target_3 : status pencapaian target kategori 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032034,
     "end_time": "2021-01-04T09:44:47.954427",
     "exception": false,
     "start_time": "2021-01-04T09:44:47.922393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Grouped Features based on Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03079,
     "end_time": "2021-01-04T09:44:48.016864",
     "exception": false,
     "start_time": "2021-01-04T09:44:47.986074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**int64**\n",
    "* age                                        int64\n",
    "* number_of_dependences                      int64\n",
    "* number_of_dependences (male)               int64\n",
    "* number_of_dependences (female)             int64\n",
    "* job_duration_from_training                 int64\n",
    "* branch_rotation                            int64\n",
    "* job_rotation                               int64\n",
    "* assign_of_otherposition                    int64\n",
    "* annual leave                               int64\n",
    "* sick_leaves                                int64\n",
    "* Best Performance                           int64\n",
    "\n",
    "\n",
    "**float64**\n",
    "* job_duration_in_current_job_level        float64\n",
    "* job_duration_in_current_person_level     float64\n",
    "* job_duration_in_current_branch           float64\n",
    "* GPA                                      float64\n",
    "* job_duration_as_permanent_worker         float64\n",
    "* Avg_achievement_%                        float64\n",
    "* Last_achievement_%                       float64\n",
    "* Achievement_above_100%_during3quartal    float64\n",
    "\n",
    "\n",
    "**object**\n",
    "* job_level                                 object\n",
    "* person_level                              object\n",
    "* Employee_type                             object\n",
    "* Employee_status                           object\n",
    "* gender                                    object\n",
    "* marital_status_maried(Y/N)                object\n",
    "* achievement_target_1                      object\n",
    "* achievement_target_2                      object\n",
    "* achievement_target_3                      object\n",
    "* year_graduated                            object\n",
    "* Education_level                           object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031247,
     "end_time": "2021-01-04T09:44:48.079509",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.048262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# What to Do ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031183,
     "end_time": "2021-01-04T09:44:48.141943",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.110760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Data Cleaning**\n",
    "* Handling Missing Values\n",
    "* Scaling and Normalizations\n",
    "* Parsing Dates\n",
    "* Character Encoding\n",
    "* Inconsistent Data Entry\n",
    "\n",
    "**Intermediate Machine Learning**\n",
    "* Missing Values\n",
    "* Categorical Variables\n",
    "* Pipelines\n",
    "* Cross Validation\n",
    "* XG Boost\n",
    "* Data Leakage\n",
    "\n",
    "\n",
    "**Feature Engineering**\n",
    "* Baseline Model\n",
    "* Categorical Encodings\n",
    "* Feature Generation\n",
    "* Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030416,
     "end_time": "2021-01-04T09:44:48.204867",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.174451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DIREKTORI DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:48.273862Z",
     "iopub.status.busy": "2021-01-04T09:44:48.273168Z",
     "iopub.status.idle": "2021-01-04T09:44:48.305993Z",
     "shell.execute_reply": "2021-01-04T09:44:48.305206Z"
    },
    "papermill": {
     "duration": 0.070651,
     "end_time": "2021-01-04T09:44:48.306119",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.235468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-people/train_ahmad.csv\n",
      "/kaggle/input/data-people/train.csv\n",
      "/kaggle/input/data-people/test.csv\n",
      "/kaggle/input/bri-data-hackathon-people-analytic/sample_submission.csv\n",
      "/kaggle/input/bri-data-hackathon-people-analytic/train.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Read datasets\n",
    "train_dir = '/kaggle/input/data-people/train.csv'\n",
    "test_dir = '/kaggle/input/data-people/test.csv'\n",
    "submission_dir = '/kaggle/input/bri-data-hackathon-people-analytic/sample_submission.csv'\n",
    "        \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031513,
     "end_time": "2021-01-04T09:44:48.370713",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.339200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030815,
     "end_time": "2021-01-04T09:44:48.433091",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.402276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "List of features that have missing data:\n",
    "* Employee_type = Assume MCAR\n",
    "* Education_level = Assume MNAR\n",
    "* GPA = Assume MAR\n",
    "* Year_graduated = Assume MAR\n",
    "* Job_duration_as_permanent_worker = Assume MAR\n",
    "* Avg_achievement_% = Assume MAR\n",
    "* Last_achievement_% = Assume MAR\n",
    "* Achievement_above_100%_during3quartal = Assume MAR\n",
    "* achievement_target_1 = Assume MAR\n",
    "* achievement_target_2 = Assume MAR\n",
    "* achievement_target_3 = Assume MAR\n",
    "\n",
    "\n",
    "30 Desember 2020\n",
    "* KNN ga bisa handle feature categorical \n",
    "* Consider Few Scenarios for missing values:\n",
    "* Ordinal Encoding, Impute with KNN\n",
    "* Ordinal Encoding, Impute with MICE\n",
    "* Ordinal Encoding, Impute with datawig\n",
    "* Ordinal Encoding, Impute with LDA\n",
    "* OHE, Impute with ?\n",
    "\n",
    "* Beware of Data Leakage\n",
    "* Fit hanya di training jangan di validation \n",
    "* Apakah perlu imputasi di validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031157,
     "end_time": "2021-01-04T09:44:48.495817",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.464660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Split Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:48.567172Z",
     "iopub.status.busy": "2021-01-04T09:44:48.566353Z",
     "iopub.status.idle": "2021-01-04T09:44:49.945026Z",
     "shell.execute_reply": "2021-01-04T09:44:49.943412Z"
    },
    "papermill": {
     "duration": 1.41774,
     "end_time": "2021-01-04T09:44:49.945157",
     "exception": false,
     "start_time": "2021-01-04T09:44:48.527417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the Data \n",
    "X_full = pd.read_csv(train_dir)\n",
    "X_test_full = pd.read_csv(test_dir)\n",
    "\n",
    "# Separate target from predictors\n",
    "y = X_full['Best Performance']\n",
    "X_full.drop(['Best Performance'], axis=1, inplace=True)\n",
    "X_CV = X_full\n",
    "y_CV = y\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031279,
     "end_time": "2021-01-04T09:44:50.010353",
     "exception": false,
     "start_time": "2021-01-04T09:44:49.979074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031389,
     "end_time": "2021-01-04T09:44:50.073421",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.042032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST SMALL SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:50.146356Z",
     "iopub.status.busy": "2021-01-04T09:44:50.145280Z",
     "iopub.status.idle": "2021-01-04T09:44:50.150356Z",
     "shell.execute_reply": "2021-01-04T09:44:50.149867Z"
    },
    "papermill": {
     "duration": 0.045477,
     "end_time": "2021-01-04T09:44:50.150481",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.105004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#print(X_full['achievement_target_3'].unique())\\n# Test Perbaikin inkonsisten\\nnonulls = np.array(X_train_full['year_graduated'].dropna())\\nfor i in range(len(nonulls)):\\n    if (nonulls[i] == '____') |  (nonulls[i] == '-') |  (nonulls[i] == '\\\\N'):\\n            nonulls[i] = np.nan\\n    elif (int(nonulls[i]) > 2020) | (int(nonulls[i]) < 1900):\\n            nonulls[i] = np.nan\\n    else:\\n            nonulls[i] = int(nonulls[i])\\nrshp_nn = nonulls.reshape(-1,1)\\n#print(rshp_nn.shape)\\nX_train_full['year_graduated'].loc[X_train_full['year_graduated'].notnull()] = np.squeeze(rshp_nn)\\nX_train_full['year_graduated'] = pd.to_numeric(X_train_full['year_graduated'])\\nX_train_full['year_graduated']\\n#X_train_full['year_graduated'].loc[X_train_full['year_graduated'].notnull()] = np.squeeze(rshp_nn)\\n# Problem abis dipisah traning sama valid, index jadi kacau\\n# Ada dua pilihan benerin index / pake encoder otomatis\\n# achievement_target_1: Tidak konsisten\\n# achievement_target_2: Tidak konsisten\\n# achievement_target_3: Tidak konsisten\\n# year_graduated: Tidak konsisten, bisa jadi numerikal\\n# Age : Umur\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#print(X_full['achievement_target_3'].unique())\n",
    "# Test Perbaikin inkonsisten\n",
    "nonulls = np.array(X_train_full['year_graduated'].dropna())\n",
    "for i in range(len(nonulls)):\n",
    "    if (nonulls[i] == '____') |  (nonulls[i] == '-') |  (nonulls[i] == '\\\\N'):\n",
    "            nonulls[i] = np.nan\n",
    "    elif (int(nonulls[i]) > 2020) | (int(nonulls[i]) < 1900):\n",
    "            nonulls[i] = np.nan\n",
    "    else:\n",
    "            nonulls[i] = int(nonulls[i])\n",
    "rshp_nn = nonulls.reshape(-1,1)\n",
    "#print(rshp_nn.shape)\n",
    "X_train_full['year_graduated'].loc[X_train_full['year_graduated'].notnull()] = np.squeeze(rshp_nn)\n",
    "X_train_full['year_graduated'] = pd.to_numeric(X_train_full['year_graduated'])\n",
    "X_train_full['year_graduated']\n",
    "#X_train_full['year_graduated'].loc[X_train_full['year_graduated'].notnull()] = np.squeeze(rshp_nn)\n",
    "# Problem abis dipisah traning sama valid, index jadi kacau\n",
    "# Ada dua pilihan benerin index / pake encoder otomatis\n",
    "# achievement_target_1: Tidak konsisten\n",
    "# achievement_target_2: Tidak konsisten\n",
    "# achievement_target_3: Tidak konsisten\n",
    "# year_graduated: Tidak konsisten, bisa jadi numerikal\n",
    "# Age : Umur\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035295,
     "end_time": "2021-01-04T09:44:50.220845",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.185550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEST LARGE SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:50.294892Z",
     "iopub.status.busy": "2021-01-04T09:44:50.293955Z",
     "iopub.status.idle": "2021-01-04T09:44:50.298089Z",
     "shell.execute_reply": "2021-01-04T09:44:50.297449Z"
    },
    "papermill": {
     "duration": 0.045311,
     "end_time": "2021-01-04T09:44:50.298198",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.252887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# achievement_target_1: Tidak konsisten\\nimport re\\n#print(X_train_full['achievement_target_1'].value_counts())\\ndef consist_achv1 (data_achv1):\\n    nonulls = np.array(data_achv1.dropna())\\n    for i in range(len(nonulls)):\\n        if re.findall('< 50%$',nonulls[i]):\\n            nonulls[i] = '<50%'\\n        elif re.findall('50%-100%$',nonulls[i]):\\n            nonulls[i] = '50%-100%'\\n        elif re.findall('100%-150%$',nonulls[i]):\\n            nonulls[i] = '100%-150%'\\n        elif re.findall('> 1.5$',nonulls[i]):\\n            nonulls[i] = '>150%'\\n        else :\\n            nonulls[i] = 'No_target'\\n    rshp_nn = nonulls.reshape(-1,1)\\n    data_achv1.loc[data_achv1.notnull()] = np.squeeze(rshp_nn)\\n    return data_achv1\\n#X_train_full['achievement_target_1'] = consist_achv1(X_train_full['achievement_target_1'])\\n#print(X_train_full['achievement_target_1'].value_counts())\\n#TEST SUCCESS\\n\\n# achievement_target_2: Tidak konsisten\\n#print(X_train_full['achievement_target_2'].value_counts())\\ndef consist_achv2 (data_achv2):\\n    nonulls = np.array(data_achv2.dropna())\\n    for i in range(len(nonulls)):\\n        if re.findall('< 50%$',nonulls[i]):\\n            nonulls[i] = '<50%'\\n        elif re.findall('50%-100%$',nonulls[i]):\\n            nonulls[i] = '50%-100%'\\n        elif re.findall('100%-150%$',nonulls[i]):\\n            nonulls[i] = '100%-150%'\\n        elif re.findall('> 1.5$',nonulls[i]):\\n            nonulls[i] = '>150%'\\n        else :\\n            nonulls[i] = 'No_target'\\n    rshp_nn = nonulls.reshape(-1,1)\\n    data_achv2.loc[data_achv2.notnull()] = np.squeeze(rshp_nn)\\n    return data_achv2\\n#X_train_full['achievement_target_2'] = consist_achv2(X_train_full['achievement_target_2'])\\n#print(X_train_full['achievement_target_2'].value_counts())\\n#TEST SUCCESS\\n\\n# achievement_target_3: Tidak konsisten\\n#print(X_train_full['achievement_target_3'].value_counts())\\ndef consist_achv3 (data_achv3):\\n    nonulls = np.array(data_achv3.dropna())\\n    for i in range(len(nonulls)):\\n        if re.findall('^reached',nonulls[i]):\\n            nonulls[i] = 'yes'\\n        else :\\n            nonulls[i] = 'no'\\n    rshp_nn = nonulls.reshape(-1,1)\\n    data_achv3.loc[data_achv3.notnull()] = np.squeeze(rshp_nn)\\n    return data_achv3\\n#X_train_full['achievement_target_3'] = consist_achv3(X_train_full['achievement_target_3'])\\n#print(X_train_full['achievement_target_3'].value_counts())\\n#TEST SUCCESS\\n\\n# year_graduated: Tidak konsisten, bisa jadi numerikal\\n#print(X_train_full['year_graduated'].unique())\\ndef consist_year_grad (data_year_grad):\\n    nonulls = np.array(data_year_grad.dropna())\\n    for i in range(len(nonulls)):\\n        if (nonulls[i] == '____') |  (nonulls[i] == '-') |  (nonulls[i] == '\\\\N'):\\n                nonulls[i] = np.nan\\n        elif (int(nonulls[i]) > 2020) | (int(nonulls[i]) < 1900):\\n                nonulls[i] = np.nan\\n        else:\\n                nonulls[i] = int(nonulls[i])\\n    rshp_nn = nonulls.reshape(-1,1)\\n    data_year_grad.loc[data_year_grad.notnull()] = np.squeeze(rshp_nn)\\n    data_year_grad = pd.to_numeric(data_year_grad)\\n    return data_year_grad\\n#X_train_full['year_graduated'] = consist_year_grad(X_train_full['year_graduated'])\\n#print(X_train_full['year_graduated'].unique())\\n#TEST SUCCESS\\n\\n# age: ubah tahun lahir jadi umur\\ndef fix_age (data_age):\\n    nonulls = np.array(data_age.dropna())\\n    for i in range(len(nonulls)):\\n        nonulls[i] = 2020 - nonulls[i]\\n    rshp_nn = nonulls.reshape(-1,1)\\n    data_age.loc[data_age.notnull()] = np.squeeze(rshp_nn)\\n    return data_age\\n#X_train_full['age'] = fix_age(X_train_full['age'])\\n#X_train_full['age'].value_counts()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# achievement_target_1: Tidak konsisten\n",
    "import re\n",
    "#print(X_train_full['achievement_target_1'].value_counts())\n",
    "def consist_achv1 (data_achv1):\n",
    "    nonulls = np.array(data_achv1.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if re.findall('< 50%$',nonulls[i]):\n",
    "            nonulls[i] = '<50%'\n",
    "        elif re.findall('50%-100%$',nonulls[i]):\n",
    "            nonulls[i] = '50%-100%'\n",
    "        elif re.findall('100%-150%$',nonulls[i]):\n",
    "            nonulls[i] = '100%-150%'\n",
    "        elif re.findall('> 1.5$',nonulls[i]):\n",
    "            nonulls[i] = '>150%'\n",
    "        else :\n",
    "            nonulls[i] = 'No_target'\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_achv1.loc[data_achv1.notnull()] = np.squeeze(rshp_nn)\n",
    "    return data_achv1\n",
    "#X_train_full['achievement_target_1'] = consist_achv1(X_train_full['achievement_target_1'])\n",
    "#print(X_train_full['achievement_target_1'].value_counts())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# achievement_target_2: Tidak konsisten\n",
    "#print(X_train_full['achievement_target_2'].value_counts())\n",
    "def consist_achv2 (data_achv2):\n",
    "    nonulls = np.array(data_achv2.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if re.findall('< 50%$',nonulls[i]):\n",
    "            nonulls[i] = '<50%'\n",
    "        elif re.findall('50%-100%$',nonulls[i]):\n",
    "            nonulls[i] = '50%-100%'\n",
    "        elif re.findall('100%-150%$',nonulls[i]):\n",
    "            nonulls[i] = '100%-150%'\n",
    "        elif re.findall('> 1.5$',nonulls[i]):\n",
    "            nonulls[i] = '>150%'\n",
    "        else :\n",
    "            nonulls[i] = 'No_target'\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_achv2.loc[data_achv2.notnull()] = np.squeeze(rshp_nn)\n",
    "    return data_achv2\n",
    "#X_train_full['achievement_target_2'] = consist_achv2(X_train_full['achievement_target_2'])\n",
    "#print(X_train_full['achievement_target_2'].value_counts())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# achievement_target_3: Tidak konsisten\n",
    "#print(X_train_full['achievement_target_3'].value_counts())\n",
    "def consist_achv3 (data_achv3):\n",
    "    nonulls = np.array(data_achv3.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if re.findall('^reached',nonulls[i]):\n",
    "            nonulls[i] = 'yes'\n",
    "        else :\n",
    "            nonulls[i] = 'no'\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_achv3.loc[data_achv3.notnull()] = np.squeeze(rshp_nn)\n",
    "    return data_achv3\n",
    "#X_train_full['achievement_target_3'] = consist_achv3(X_train_full['achievement_target_3'])\n",
    "#print(X_train_full['achievement_target_3'].value_counts())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# year_graduated: Tidak konsisten, bisa jadi numerikal\n",
    "#print(X_train_full['year_graduated'].unique())\n",
    "def consist_year_grad (data_year_grad):\n",
    "    nonulls = np.array(data_year_grad.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if (nonulls[i] == '____') |  (nonulls[i] == '-') |  (nonulls[i] == '\\\\N'):\n",
    "                nonulls[i] = np.nan\n",
    "        elif (int(nonulls[i]) > 2020) | (int(nonulls[i]) < 1900):\n",
    "                nonulls[i] = np.nan\n",
    "        else:\n",
    "                nonulls[i] = int(nonulls[i])\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_year_grad.loc[data_year_grad.notnull()] = np.squeeze(rshp_nn)\n",
    "    data_year_grad = pd.to_numeric(data_year_grad)\n",
    "    return data_year_grad\n",
    "#X_train_full['year_graduated'] = consist_year_grad(X_train_full['year_graduated'])\n",
    "#print(X_train_full['year_graduated'].unique())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# age: ubah tahun lahir jadi umur\n",
    "def fix_age (data_age):\n",
    "    nonulls = np.array(data_age.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        nonulls[i] = 2020 - nonulls[i]\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_age.loc[data_age.notnull()] = np.squeeze(rshp_nn)\n",
    "    return data_age\n",
    "#X_train_full['age'] = fix_age(X_train_full['age'])\n",
    "#X_train_full['age'].value_counts()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032417,
     "end_time": "2021-01-04T09:44:50.363512",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.331095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPLEMENTASI BERMASALAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03224,
     "end_time": "2021-01-04T09:44:50.428518",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.396278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "NGGA BISA PERBAIKIN DATA INKONSISTEN KALO ADA NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:50.505016Z",
     "iopub.status.busy": "2021-01-04T09:44:50.504311Z",
     "iopub.status.idle": "2021-01-04T09:44:50.508742Z",
     "shell.execute_reply": "2021-01-04T09:44:50.508227Z"
    },
    "papermill": {
     "duration": 0.047268,
     "end_time": "2021-01-04T09:44:50.508853",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.461585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport re\\ndef clean_incon (data) :\\n    # Encoding Job_level\\n    for i in range(len(data)):\\n        if \\'03\\' in data[\\'job_level\\'][i]:\\n            data[\\'job_level\\'][i] = 0\\n        elif \\'04\\' in data[\\'job_level\\'][i]:\\n            data[\\'job_level\\'][i] = 1\\n        elif \\'05\\' in data[\\'job_level\\'][i]:\\n            data[\\'job_level\\'][i] = 2\\n        else: \\n            data[\\'job_level\\'][i] = 3\\n    \\n    # Encoding Person_level\\n    for i in range(len(data)):\\n        if \\'01\\' in data[\\'person_level\\'][i]:\\n            data[\\'person_level\\'][i] = 0\\n        elif \\'02\\' in data[\\'person_level\\'][i]:\\n            data[\\'person_level\\'][i] = 1\\n        elif \\'03\\' in data[\\'person_level\\'][i]:\\n            data[\\'person_level\\'][i] = 2\\n        elif \\'04\\' in data[\\'person_level\\'][i]:\\n            data[\\'person_level\\'][i] = 3\\n        elif \\'05\\' in data[\\'person_level\\'][i]:\\n            data[\\'person_level\\'][i] = 4\\n        elif \\'06\\' in data[\\'person_level\\'][i]:\\n            data[\\'person_level\\'][i] = 5\\n        elif \\'07\\' in data[\\'person_level\\'][i]:\\n            data[\\'person_level\\'][i] = 6\\n        else: \\n            data[\\'person_level\\'][i] = 7\\n\\n    # Encoding Employee_type\\n    for i in range(len(data)):\\n        # Ga boleh ada NA di data kalo mau pake IN\\n        if data[\\'Employee_type\\'][i] == np.nan\\n        elif \"A\" in data[\\'Employee_type\\'][i]:\\n            data[\\'Employee_type\\'][i] = 0\\n        elif \"B\" in data[\\'Employee_type\\'][i]:\\n            data[\\'Employee_type\\'][i] = 1\\n        else: \\n            data[\\'Employee_type\\'][i] = 2\\n    \\n    # Encoding Employee_status\\n    for i in range(len(data)):\\n        if data[\\'Employee_status\\'][i] == \\'Permanent\\':\\n            data[\\'Employee_status\\'][i] = 1\\n        else:\\n            data[\\'Employee_status\\'][i] = 0\\n    \\n    # Encoding Gender\\n    for i in range(len(data)):\\n        if data[\\'gender\\'][i] ==\\'Female\\':\\n            data[\\'gender\\'][i] = 1\\n        else:\\n            data[\\'gender\\'][i] = 0\\n            \\n    # Encoding marital_status_maried(Y/N)\\n    for i in range(len(data)):\\n        if (data[\\'marital_status_maried(Y/N)\\'][i] ==\\'Y\\'):\\n            data[\\'marital_status_maried(Y/N)\\'][i] = 1\\n        else :\\n            data[\\'marital_status_maried(Y/N)\\'][i] = 0\\n            \\n    # Encoding Education_level\\n    for i in range (len(data)):\\n        if re.findall(\\'4$\\',data[\\'Education_level\\'][i]):\\n            data[\\'Education_level\\'][i] = 4\\n        elif re.findall(\\'3$\\',data[\\'Education_level\\'][i]):\\n            data[\\'Education_level\\'][i] = 3\\n        elif re.findall(\\'5$\\',data[\\'Education_level\\'][i]):\\n            data[\\'Education_level\\'][i] = 5\\n        elif re.findall(\\'2$\\',data[\\'Education_level\\'][i]):\\n            data[\\'Education_level\\'][i] = 2\\n        elif re.findall(\\'1$\\',data[\\'Education_level\\'][i]):\\n            data[\\'Education_level\\'][i] = 1\\n        else :\\n            data[\\'Education_level\\'][i] = 0\\n    # year_graduated\\n    for i in range(len(data)):\\n        if (data[\\'year_graduated\\'][i] == \\'____\\') |  (data[\\'year_graduated\\'][i] == \\'-\\') |  (data[\\'year_graduated\\'][i] == \\'\\\\N\\'):\\n            data[\\'year_graduated\\'][i] = np.nan\\n        elif (int(data[\\'year_graduated\\'][i]) > 2020) | (int(data[\\'year_graduated\\'][i]) < 1900) | (int(data[\\'year_graduated\\'][i]) < (data[\\'age\\'][i])):\\n            data[\\'year_graduated\\'][i] = np.nan\\n        else:\\n            data[\\'year_graduated\\'][i] = int(data[\\'year_graduated\\'][i])\\n    \\n    \\n    # achievement_target_1\\n    for i in range(len(data)):\\n        if re.findall(\\'<50%$\\',data[\\'achievement_target_1\\'][i]):\\n            data[\\'achievement_target_1\\'][i] = \\'<50\\'\\n        elif re.findall(\\'50%-100%$\\',data[\\'achievement_target_1\\'][i]):\\n            data[\\'achievement_target_1\\'][i] = \\'50%-100\\'\\n        elif re.findall(\\'100%-150%$\\',data[\\'achievement_target_1\\'][i]):\\n            data[\\'achievement_target_1\\'][i] = \\'100%-150%\\'\\n        elif re.findall(\\'> 1.5$\\',data[\\'achievement_target_1\\'][i]):\\n            data[\\'achievement_target_1\\'][i] = \\'>150%\\'\\n        else :\\n            data[\\'achievement_target_1\\'][i] = \\'No_target\\'\\n    \\n    # achievement_target_2\\n    for i in range(len(data)):\\n        if re.findall(\\'<50%$\\',data[\\'achievement_target_2\\'][i]):\\n            data[\\'achievement_target_2\\'][i] = \\'<50\\'\\n        elif re.findall(\\'50%-100%$\\',data[\\'achievement_target_2\\'][i]):\\n            data[\\'achievement_target_2\\'][i] = \\'50%-100\\'\\n        elif re.findall(\\'100%-150%$\\',data[\\'achievement_target_2\\'][i]):\\n            data[\\'achievement_target_2\\'][i] = \\'100%-150%\\'\\n        elif re.findall(\\'> 1.5$\\',data[\\'achievement_target_2\\'][i]):\\n            data[\\'achievement_target_2\\'][i] = \\'>150%\\'\\n        else :\\n            data[\\'achievement_target_2\\'][i] = \\'No_target\\'\\n    \\n    # achievement_target_3\\n    for i in range(len(data)):\\n        if re.findall(\\'^reached\\',data[\\'achievement_target_3\\'][i]):\\n            data[\\'achievement_target_3\\'][i] = \\'yes\\'\\n        else :\\n            data[\\'achievement_target_3\\'][i] = \\'no\\'\\n            \\n    # Fixing Age\\n    for i in range(len(data)):\\n        data[\\'age\\'][i] = 2020 - data[\\'age\\'][i]\\n        \\n    return data\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import re\n",
    "def clean_incon (data) :\n",
    "    # Encoding Job_level\n",
    "    for i in range(len(data)):\n",
    "        if '03' in data['job_level'][i]:\n",
    "            data['job_level'][i] = 0\n",
    "        elif '04' in data['job_level'][i]:\n",
    "            data['job_level'][i] = 1\n",
    "        elif '05' in data['job_level'][i]:\n",
    "            data['job_level'][i] = 2\n",
    "        else: \n",
    "            data['job_level'][i] = 3\n",
    "    \n",
    "    # Encoding Person_level\n",
    "    for i in range(len(data)):\n",
    "        if '01' in data['person_level'][i]:\n",
    "            data['person_level'][i] = 0\n",
    "        elif '02' in data['person_level'][i]:\n",
    "            data['person_level'][i] = 1\n",
    "        elif '03' in data['person_level'][i]:\n",
    "            data['person_level'][i] = 2\n",
    "        elif '04' in data['person_level'][i]:\n",
    "            data['person_level'][i] = 3\n",
    "        elif '05' in data['person_level'][i]:\n",
    "            data['person_level'][i] = 4\n",
    "        elif '06' in data['person_level'][i]:\n",
    "            data['person_level'][i] = 5\n",
    "        elif '07' in data['person_level'][i]:\n",
    "            data['person_level'][i] = 6\n",
    "        else: \n",
    "            data['person_level'][i] = 7\n",
    "\n",
    "    # Encoding Employee_type\n",
    "    for i in range(len(data)):\n",
    "        # Ga boleh ada NA di data kalo mau pake IN\n",
    "        if data['Employee_type'][i] == np.nan\n",
    "        elif \"A\" in data['Employee_type'][i]:\n",
    "            data['Employee_type'][i] = 0\n",
    "        elif \"B\" in data['Employee_type'][i]:\n",
    "            data['Employee_type'][i] = 1\n",
    "        else: \n",
    "            data['Employee_type'][i] = 2\n",
    "    \n",
    "    # Encoding Employee_status\n",
    "    for i in range(len(data)):\n",
    "        if data['Employee_status'][i] == 'Permanent':\n",
    "            data['Employee_status'][i] = 1\n",
    "        else:\n",
    "            data['Employee_status'][i] = 0\n",
    "    \n",
    "    # Encoding Gender\n",
    "    for i in range(len(data)):\n",
    "        if data['gender'][i] =='Female':\n",
    "            data['gender'][i] = 1\n",
    "        else:\n",
    "            data['gender'][i] = 0\n",
    "            \n",
    "    # Encoding marital_status_maried(Y/N)\n",
    "    for i in range(len(data)):\n",
    "        if (data['marital_status_maried(Y/N)'][i] =='Y'):\n",
    "            data['marital_status_maried(Y/N)'][i] = 1\n",
    "        else :\n",
    "            data['marital_status_maried(Y/N)'][i] = 0\n",
    "            \n",
    "    # Encoding Education_level\n",
    "    for i in range (len(data)):\n",
    "        if re.findall('4$',data['Education_level'][i]):\n",
    "            data['Education_level'][i] = 4\n",
    "        elif re.findall('3$',data['Education_level'][i]):\n",
    "            data['Education_level'][i] = 3\n",
    "        elif re.findall('5$',data['Education_level'][i]):\n",
    "            data['Education_level'][i] = 5\n",
    "        elif re.findall('2$',data['Education_level'][i]):\n",
    "            data['Education_level'][i] = 2\n",
    "        elif re.findall('1$',data['Education_level'][i]):\n",
    "            data['Education_level'][i] = 1\n",
    "        else :\n",
    "            data['Education_level'][i] = 0\n",
    "    # year_graduated\n",
    "    for i in range(len(data)):\n",
    "        if (data['year_graduated'][i] == '____') |  (data['year_graduated'][i] == '-') |  (data['year_graduated'][i] == '\\\\N'):\n",
    "            data['year_graduated'][i] = np.nan\n",
    "        elif (int(data['year_graduated'][i]) > 2020) | (int(data['year_graduated'][i]) < 1900) | (int(data['year_graduated'][i]) < (data['age'][i])):\n",
    "            data['year_graduated'][i] = np.nan\n",
    "        else:\n",
    "            data['year_graduated'][i] = int(data['year_graduated'][i])\n",
    "    \n",
    "    \n",
    "    # achievement_target_1\n",
    "    for i in range(len(data)):\n",
    "        if re.findall('<50%$',data['achievement_target_1'][i]):\n",
    "            data['achievement_target_1'][i] = '<50'\n",
    "        elif re.findall('50%-100%$',data['achievement_target_1'][i]):\n",
    "            data['achievement_target_1'][i] = '50%-100'\n",
    "        elif re.findall('100%-150%$',data['achievement_target_1'][i]):\n",
    "            data['achievement_target_1'][i] = '100%-150%'\n",
    "        elif re.findall('> 1.5$',data['achievement_target_1'][i]):\n",
    "            data['achievement_target_1'][i] = '>150%'\n",
    "        else :\n",
    "            data['achievement_target_1'][i] = 'No_target'\n",
    "    \n",
    "    # achievement_target_2\n",
    "    for i in range(len(data)):\n",
    "        if re.findall('<50%$',data['achievement_target_2'][i]):\n",
    "            data['achievement_target_2'][i] = '<50'\n",
    "        elif re.findall('50%-100%$',data['achievement_target_2'][i]):\n",
    "            data['achievement_target_2'][i] = '50%-100'\n",
    "        elif re.findall('100%-150%$',data['achievement_target_2'][i]):\n",
    "            data['achievement_target_2'][i] = '100%-150%'\n",
    "        elif re.findall('> 1.5$',data['achievement_target_2'][i]):\n",
    "            data['achievement_target_2'][i] = '>150%'\n",
    "        else :\n",
    "            data['achievement_target_2'][i] = 'No_target'\n",
    "    \n",
    "    # achievement_target_3\n",
    "    for i in range(len(data)):\n",
    "        if re.findall('^reached',data['achievement_target_3'][i]):\n",
    "            data['achievement_target_3'][i] = 'yes'\n",
    "        else :\n",
    "            data['achievement_target_3'][i] = 'no'\n",
    "            \n",
    "    # Fixing Age\n",
    "    for i in range(len(data)):\n",
    "        data['age'][i] = 2020 - data['age'][i]\n",
    "        \n",
    "    return data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:50.583910Z",
     "iopub.status.busy": "2021-01-04T09:44:50.583114Z",
     "iopub.status.idle": "2021-01-04T09:44:50.585831Z",
     "shell.execute_reply": "2021-01-04T09:44:50.585286Z"
    },
    "papermill": {
     "duration": 0.042856,
     "end_time": "2021-01-04T09:44:50.585953",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.543097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set index from 0 to len(data) to fit in the cleaning function\n",
    "#X_train_full = X_train_full.set_index(pd.Index(list(range(len(X_train_full)))))\n",
    "#X_valid_full = X_valid_full.set_index(pd.Index(list(range(len(X_valid_full)))))\n",
    "#clean_incon(X_train_full)\n",
    "#X_train_full.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033984,
     "end_time": "2021-01-04T09:44:50.653840",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.619856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMPLEMENTASI PERBAIKAN INKONSISTENSI FINAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:50.736439Z",
     "iopub.status.busy": "2021-01-04T09:44:50.731342Z",
     "iopub.status.idle": "2021-01-04T09:44:50.758023Z",
     "shell.execute_reply": "2021-01-04T09:44:50.757462Z"
    },
    "papermill": {
     "duration": 0.070219,
     "end_time": "2021-01-04T09:44:50.758153",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.687934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pd.options.mode.chained_assignment = None\n",
    "# achievement_target_1: Tidak konsisten\n",
    "#print(X_train_full['achievement_target_1'].value_counts())\n",
    "def consist_achv1 (data_achv1):\n",
    "    nonulls = np.array(data_achv1.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if re.findall('< 50%$',nonulls[i]):\n",
    "            nonulls[i] = '<50%'\n",
    "        elif re.findall('50%-100%$',nonulls[i]):\n",
    "            nonulls[i] = '50%-100%'\n",
    "        elif re.findall('100%-150%$',nonulls[i]):\n",
    "            nonulls[i] = '100%-150%'\n",
    "        elif re.findall('> 1.5$',nonulls[i]):\n",
    "            nonulls[i] = '>150%'\n",
    "        else :\n",
    "            nonulls[i] = 'No_target'\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_achv1.loc[data_achv1.notnull()] = np.squeeze(rshp_nn)\n",
    "    return data_achv1\n",
    "#X_train_full['achievement_target_1'] = consist_achv1(X_train_full['achievement_target_1'])\n",
    "#print(X_train_full['achievement_target_1'].value_counts())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# achievement_target_2: Tidak konsisten\n",
    "#print(X_train_full['achievement_target_2'].value_counts())\n",
    "def consist_achv2 (data_achv2):\n",
    "    nonulls = np.array(data_achv2.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if re.findall('< 50%$',nonulls[i]):\n",
    "            nonulls[i] = '<50%'\n",
    "        elif re.findall('50%-100%$',nonulls[i]):\n",
    "            nonulls[i] = '50%-100%'\n",
    "        elif re.findall('100%-150%$',nonulls[i]):\n",
    "            nonulls[i] = '100%-150%'\n",
    "        elif re.findall('> 1.5$',nonulls[i]):\n",
    "            nonulls[i] = '>150%'\n",
    "        else :\n",
    "            nonulls[i] = 'No_target'\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_achv2.loc[data_achv2.notnull()] = np.squeeze(rshp_nn)\n",
    "    return data_achv2\n",
    "#X_train_full['achievement_target_2'] = consist_achv2(X_train_full['achievement_target_2'])\n",
    "#print(X_train_full['achievement_target_2'].value_counts())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# achievement_target_3: Tidak konsisten\n",
    "#print(X_train_full['achievement_target_3'].value_counts())\n",
    "def consist_achv3 (data_achv3):\n",
    "    nonulls = np.array(data_achv3.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if re.findall('^reached',nonulls[i]):\n",
    "            nonulls[i] = 'yes'\n",
    "        else :\n",
    "            nonulls[i] = 'no'\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_achv3.loc[data_achv3.notnull()] = np.squeeze(rshp_nn)\n",
    "    #return data_achv3\n",
    "#X_train_full['achievement_target_3'] = consist_achv3(X_train_full['achievement_target_3'])\n",
    "#print(X_train_full['achievement_target_3'].value_counts())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# year_graduated: Tidak konsisten, bisa jadi numerikal\n",
    "#print(X_train_full['year_graduated'].unique())\n",
    "def consist_year_grad (data_year_grad):\n",
    "    nonulls = np.array(data_year_grad.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        if (nonulls[i] == '____') |  (nonulls[i] == '-') |  (nonulls[i] == '\\\\N') | ('.' in nonulls[i]):\n",
    "                nonulls[i] = np.nan\n",
    "        elif (float(nonulls[i]) > 2020.0) | (float(nonulls[i]) < 1900.0):\n",
    "                nonulls[i] = np.nan\n",
    "        else:\n",
    "                nonulls[i] = int(nonulls[i])\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_year_grad.loc[data_year_grad.notnull()] = np.squeeze(rshp_nn)\n",
    "    data_year_grad = pd.to_numeric(data_year_grad)\n",
    "    #return data_year_grad\n",
    "#X_train_full['year_graduated'] = consist_year_grad(X_train_full['year_graduated'])\n",
    "#print(X_train_full['year_graduated'].unique())\n",
    "#TEST SUCCESS\n",
    "\n",
    "# age: ubah tahun lahir jadi umur\n",
    "def fix_age (data_age):\n",
    "    nonulls = np.array(data_age.dropna())\n",
    "    for i in range(len(nonulls)):\n",
    "        nonulls[i] = 2020 - nonulls[i]\n",
    "    rshp_nn = nonulls.reshape(-1,1)\n",
    "    data_age.loc[data_age.notnull()] = np.squeeze(rshp_nn)\n",
    "    #return data_age\n",
    "#X_train_full['age'] = fix_age(X_train_full['age'])\n",
    "#X_train_full['age'].value_counts()\n",
    "\n",
    "# Fungsi General untuk fix inconsistencies\n",
    "def all_consistent (data):\n",
    "    data['achievement_target_1'] = consist_achv1(data['achievement_target_1'])\n",
    "    data['achievement_target_2'] = consist_achv2(data['achievement_target_2'])\n",
    "    data['achievement_target_3'] = consist_achv3(data['achievement_target_3'])\n",
    "    data['year_graduated'] = consist_year_grad(data['year_graduated'])\n",
    "    data['age'] = fix_age(data['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:50.830143Z",
     "iopub.status.busy": "2021-01-04T09:44:50.829416Z",
     "iopub.status.idle": "2021-01-04T09:44:51.333816Z",
     "shell.execute_reply": "2021-01-04T09:44:51.333125Z"
    },
    "papermill": {
     "duration": 0.541648,
     "end_time": "2021-01-04T09:44:51.333927",
     "exception": false,
     "start_time": "2021-01-04T09:44:50.792279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Fix The inconsistencies\n",
    "all_consistent(X_train_full)\n",
    "all_consistent(X_valid_full)\n",
    "all_consistent(X_test_full)\n",
    "all_consistent(X_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:51.408930Z",
     "iopub.status.busy": "2021-01-04T09:44:51.407890Z",
     "iopub.status.idle": "2021-01-04T09:44:51.411931Z",
     "shell.execute_reply": "2021-01-04T09:44:51.412393Z"
    },
    "papermill": {
     "duration": 0.044676,
     "end_time": "2021-01-04T09:44:51.412556",
     "exception": false,
     "start_time": "2021-01-04T09:44:51.367880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Pada data training\\nX_train_full['achievement_target_1'] = consist_achv1(X_train_full['achievement_target_1'])\\nX_train_full['achievement_target_2'] = consist_achv2(X_train_full['achievement_target_2'])\\nX_train_full['achievement_target_3'] = consist_achv3(X_train_full['achievement_target_3'])\\nX_train_full['year_graduated'] = consist_year_grad(X_train_full['year_graduated'])\\nX_train_full['age'] = fix_age(X_train_full['age'])\\n#for cols in X_train_full.columns:\\n#    print(X_train_full[cols].unique())\\n# Pada data validasi\\nX_valid_full['achievement_target_1'] = consist_achv1(X_valid_full['achievement_target_1'])\\nX_valid_full['achievement_target_2'] = consist_achv2(X_valid_full['achievement_target_2'])\\nX_valid_full['achievement_target_3'] = consist_achv3(X_valid_full['achievement_target_3'])\\nX_valid_full['year_graduated'] = consist_year_grad(X_valid_full['year_graduated'])\\nX_valid_full['age'] = fix_age(X_valid_full['age'])\\n# Pada data validasi\\nX_test_full['achievement_target_1'] = consist_achv1(X_test_full['achievement_target_1'])\\nX_test_full['achievement_target_2'] = consist_achv2(X_test_full['achievement_target_2'])\\nX_test_full['achievement_target_3'] = consist_achv3(X_test_full['achievement_target_3'])\\nX_test_full['year_graduated'] = consist_year_grad(X_test_full['year_graduated'])\\nX_test_full['age'] = fix_age(X_test_full['age'])\\n#for cols in X_valid_full.columns:\\n#    print(X_valid_full[cols].unique())\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Pada data training\n",
    "X_train_full['achievement_target_1'] = consist_achv1(X_train_full['achievement_target_1'])\n",
    "X_train_full['achievement_target_2'] = consist_achv2(X_train_full['achievement_target_2'])\n",
    "X_train_full['achievement_target_3'] = consist_achv3(X_train_full['achievement_target_3'])\n",
    "X_train_full['year_graduated'] = consist_year_grad(X_train_full['year_graduated'])\n",
    "X_train_full['age'] = fix_age(X_train_full['age'])\n",
    "#for cols in X_train_full.columns:\n",
    "#    print(X_train_full[cols].unique())\n",
    "# Pada data validasi\n",
    "X_valid_full['achievement_target_1'] = consist_achv1(X_valid_full['achievement_target_1'])\n",
    "X_valid_full['achievement_target_2'] = consist_achv2(X_valid_full['achievement_target_2'])\n",
    "X_valid_full['achievement_target_3'] = consist_achv3(X_valid_full['achievement_target_3'])\n",
    "X_valid_full['year_graduated'] = consist_year_grad(X_valid_full['year_graduated'])\n",
    "X_valid_full['age'] = fix_age(X_valid_full['age'])\n",
    "# Pada data validasi\n",
    "X_test_full['achievement_target_1'] = consist_achv1(X_test_full['achievement_target_1'])\n",
    "X_test_full['achievement_target_2'] = consist_achv2(X_test_full['achievement_target_2'])\n",
    "X_test_full['achievement_target_3'] = consist_achv3(X_test_full['achievement_target_3'])\n",
    "X_test_full['year_graduated'] = consist_year_grad(X_test_full['year_graduated'])\n",
    "X_test_full['age'] = fix_age(X_test_full['age'])\n",
    "#for cols in X_valid_full.columns:\n",
    "#    print(X_valid_full[cols].unique())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034528,
     "end_time": "2021-01-04T09:44:51.481594",
     "exception": false,
     "start_time": "2021-01-04T09:44:51.447066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ENCODE AND IMPUTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:51.554555Z",
     "iopub.status.busy": "2021-01-04T09:44:51.553575Z",
     "iopub.status.idle": "2021-01-04T09:44:51.611872Z",
     "shell.execute_reply": "2021-01-04T09:44:51.612367Z"
    },
    "papermill": {
     "duration": 0.096463,
     "end_time": "2021-01-04T09:44:51.612524",
     "exception": false,
     "start_time": "2021-01-04T09:44:51.516061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of Numerical Columns\n",
    "# Numerical cols with missing data\n",
    "num_mis_cols = [cols for cols in X_train_full.columns if\n",
    "                  (X_train_full[cols].dtypes in ['int64', 'float64']) & (X_train_full[cols].isna().sum() != 0)]\n",
    "# Numerical cols without missing data\n",
    "num_nomis_cols = [cols for cols in X_train_full.columns if\n",
    "                  (X_train_full[cols].dtypes in ['int64', 'float64']) & (X_train_full[cols].isna().sum() == 0)]\n",
    "# List of Categorical Columns\n",
    "# Nominal\n",
    "nom_cols = ['Employee_type','Employee_status','gender','marital_status_maried(Y/N)']\n",
    "# Ordinal\n",
    "ord_cols = ['job_level','person_level','achievement_target_1',\n",
    "            'achievement_target_2','achievement_target_3', 'Education_level']\n",
    "# All of the numerical cols\n",
    "num_cols = num_mis_cols + num_nomis_cols\n",
    "# All Categorical Columns\n",
    "cat_cols = nom_cols + ord_cols\n",
    "# All columns\n",
    "all_cols = cat_cols + num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035135,
     "end_time": "2021-01-04T09:44:51.682205",
     "exception": false,
     "start_time": "2021-01-04T09:44:51.647070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. CATBOOST (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:51.758715Z",
     "iopub.status.busy": "2021-01-04T09:44:51.757681Z",
     "iopub.status.idle": "2021-01-04T09:44:53.346012Z",
     "shell.execute_reply": "2021-01-04T09:44:53.346494Z"
    },
    "papermill": {
     "duration": 1.628573,
     "end_time": "2021-01-04T09:44:53.346671",
     "exception": false,
     "start_time": "2021-01-04T09:44:51.718098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode using Catboost\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "CBE_encoder = CatBoostEncoder()\n",
    "X_train_full[cat_cols] = CBE_encoder.fit_transform(X_train_full[cat_cols], y_train)\n",
    "X_valid_full[cat_cols] = CBE_encoder.transform(X_valid_full[cat_cols])\n",
    "X_test_full[cat_cols] = CBE_encoder.transform(X_test_full[cat_cols])\n",
    "X_CV[cat_cols] = CBE_encoder.fit_transform(X_CV[cat_cols], y_CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034508,
     "end_time": "2021-01-04T09:44:53.416299",
     "exception": false,
     "start_time": "2021-01-04T09:44:53.381791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. KNN(Numerical) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:53.496195Z",
     "iopub.status.busy": "2021-01-04T09:44:53.495513Z",
     "iopub.status.idle": "2021-01-04T09:44:53.706359Z",
     "shell.execute_reply": "2021-01-04T09:44:53.705777Z"
    },
    "papermill": {
     "duration": 0.255548,
     "end_time": "2021-01-04T09:44:53.706482",
     "exception": false,
     "start_time": "2021-01-04T09:44:53.450934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalisasi Dulu Baru KNN\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "def normalisasi (num_data):\n",
    "    nonulls = np.array(num_data.dropna())\n",
    "    rshp_nn = nonulls.reshape(-1,1) # normalize expect 2D array\n",
    "    norm_feature = preprocessing.normalize(rshp_nn, norm='l2', axis = 0)\n",
    "    norm_feature = norm_feature.reshape(-1,1)\n",
    "    num_data.loc[num_data.notnull()] = np.squeeze(norm_feature)\n",
    "    return num_data\n",
    "for cols in all_cols:\n",
    "    X_train_full[cols] = normalisasi(X_train_full[cols])\n",
    "    X_valid_full[cols] = normalisasi(X_valid_full[cols])\n",
    "    X_test_full[cols] = normalisasi(X_test_full[cols])\n",
    "    X_CV[cols] = normalisasi(X_CV[cols])\n",
    "    #Any leakage ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:44:53.784289Z",
     "iopub.status.busy": "2021-01-04T09:44:53.783416Z",
     "iopub.status.idle": "2021-01-04T09:46:03.194048Z",
     "shell.execute_reply": "2021-01-04T09:46:03.194907Z"
    },
    "papermill": {
     "duration": 69.452988,
     "end_time": "2021-01-04T09:46:03.195092",
     "exception": false,
     "start_time": "2021-01-04T09:44:53.742104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Impute the Missing cols\n",
    "#from fancyimpute import KNN \n",
    "from sklearn.impute import KNNImputer\n",
    "import math \n",
    "knn_imputer = KNNImputer(n_neighbors = int(math.sqrt(len(X_train_full))) - 1)\n",
    "X_train_full = knn_imputer.fit_transform(X_train_full)\n",
    "X_valid_full = knn_imputer.transform(X_valid_full)\n",
    "X_CV = knn_imputer.fit_transform(X_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:03.274975Z",
     "iopub.status.busy": "2021-01-04T09:46:03.273978Z",
     "iopub.status.idle": "2021-01-04T09:46:03.277103Z",
     "shell.execute_reply": "2021-01-04T09:46:03.276356Z"
    },
    "papermill": {
     "duration": 0.046337,
     "end_time": "2021-01-04T09:46:03.277221",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.230884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "X_train_full = pd.DataFrame(X_train_full, columns = all_cols)\n",
    "X_valid_full = pd.DataFrame(X_valid_full, columns = all_cols)\n",
    "X_CV = pd.DataFrame(X_CV, columns = all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:03.355451Z",
     "iopub.status.busy": "2021-01-04T09:46:03.354551Z",
     "iopub.status.idle": "2021-01-04T09:46:03.357564Z",
     "shell.execute_reply": "2021-01-04T09:46:03.357037Z"
    },
    "papermill": {
     "duration": 0.043499,
     "end_time": "2021-01-04T09:46:03.357676",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.314177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X_valid_full.isna().sum()\n",
    "# TEST SUCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036852,
     "end_time": "2021-01-04T09:46:03.431692",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.394840",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMBALANCE DATA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036069,
     "end_time": "2021-01-04T09:46:03.504363",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.468294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "How to encounter :\n",
    "1. Class weight in Tree Based Classifier\n",
    "2. Undersample / Oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:03.584261Z",
     "iopub.status.busy": "2021-01-04T09:46:03.583136Z",
     "iopub.status.idle": "2021-01-04T09:46:03.587978Z",
     "shell.execute_reply": "2021-01-04T09:46:03.587343Z"
    },
    "papermill": {
     "duration": 0.04735,
     "end_time": "2021-01-04T09:46:03.588087",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.540737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#instantiate both packages to use\\nencoder = OrdinalEncoder()\\nimputer = KNN()\\n# create a list of categorical columns to iterate over\\n#cat_cols = ['job_level','person_level',\\n#            ,'achievement_target_1','achievement_target_2','achievement_target_3'\\n#            , 'Education_level']\\n\\ndef encode(data):\\n    function to encode non-null data and replace it in the original data\\n    #retains only non-null values\\n    nonulls = np.array(data.dropna())\\n    #reshapes the data for encoding\\n    impute_reshape = nonulls.reshape(-1,1)\\n    #encode date\\n    impute_ordinal = encoder.fit_transform(impute_reshape)\\n    #Assign back encoded values to non-null values\\n    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\\n    return data\\n\\n#create a for loop to iterate through each column in the data\\nfor columns in cat_cols:\\n    encode(impute_data[columns])\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#instantiate both packages to use\n",
    "encoder = OrdinalEncoder()\n",
    "imputer = KNN()\n",
    "# create a list of categorical columns to iterate over\n",
    "#cat_cols = ['job_level','person_level',\n",
    "#            ,'achievement_target_1','achievement_target_2','achievement_target_3'\n",
    "#            , 'Education_level']\n",
    "\n",
    "def encode(data):\n",
    "    function to encode non-null data and replace it in the original data\n",
    "    #retains only non-null values\n",
    "    nonulls = np.array(data.dropna())\n",
    "    #reshapes the data for encoding\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    #encode date\n",
    "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
    "    #Assign back encoded values to non-null values\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    return data\n",
    "\n",
    "#create a for loop to iterate through each column in the data\n",
    "for columns in cat_cols:\n",
    "    encode(impute_data[columns])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:03.669888Z",
     "iopub.status.busy": "2021-01-04T09:46:03.668874Z",
     "iopub.status.idle": "2021-01-04T09:46:03.674034Z",
     "shell.execute_reply": "2021-01-04T09:46:03.673470Z"
    },
    "papermill": {
     "duration": 0.049432,
     "end_time": "2021-01-04T09:46:03.674143",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.624711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# Handle MCAR Missing Problems\\n# Employee_type\\n#row_with_missing_employee_type = X_full[X_full[\\'Employee_type\\'].isnull()].index.tolist()\\n#X_full = X_full.drop(row_with_missing_employee_type, axis = 0)\\n\\n# Handle MAR and MNAR Missing Problems\\n# Education_level\\n#row_with_missing_education_level = X_full[X_full[\\'Education_level\\'].isnull()].index.tolist()\\n#from fancyimpute import IterativeImputer\\n#mice_imputer = IterativeImputer()\\n#X_full = mice_imputer.fit_transform(X_full)\\n#print(X_full.loc[row_with_missing_education_level, \\'Education_level\\'])\\n\\n# Select categorical columns\\ncategorical_cols = [cname for cname in X_train_full.columns if \\n                    X_train_full[cname].dtype == \"object\"]\\n\\n# Select numerical columns\\nnumerical_cols = [cname for cname in X_train_full.columns if \\n                X_train_full[cname].dtype in [\\'int64\\', \\'float64\\']]\\n# Preprocessing for numerical data\\nnumerical_transformer = SimpleImputer(strategy=\\'constant\\')\\n\\n# Preprocessing for categorical data\\ncategorical_transformer = Pipeline(steps=[\\n    (\\'imputer\\', SimpleImputer(strategy=\\'most_frequent\\')),\\n    (\\'onehot\\', OneHotEncoder(handle_unknown=\\'ignore\\'))\\n])\\n\\n# Bundle preprocessing for numerical and categorical data\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\\'num\\', numerical_transformer, numerical_cols),\\n        (\\'cat\\', categorical_transformer, categorical_cols)\\n    ])\\n\\n# Bundle preprocessing for numerical and categorical data\\n#preprocessor = ColumnTransformer(\\n#    transformers=[\\n#        (\\'num\\', numerical_transformer,numerical_cols)\\n#    ])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Handle MCAR Missing Problems\n",
    "# Employee_type\n",
    "#row_with_missing_employee_type = X_full[X_full['Employee_type'].isnull()].index.tolist()\n",
    "#X_full = X_full.drop(row_with_missing_employee_type, axis = 0)\n",
    "\n",
    "# Handle MAR and MNAR Missing Problems\n",
    "# Education_level\n",
    "#row_with_missing_education_level = X_full[X_full['Education_level'].isnull()].index.tolist()\n",
    "#from fancyimpute import IterativeImputer\n",
    "#mice_imputer = IterativeImputer()\n",
    "#X_full = mice_imputer.fit_transform(X_full)\n",
    "#print(X_full.loc[row_with_missing_education_level, 'Education_level'])\n",
    "\n",
    "# Select categorical columns\n",
    "categorical_cols = [cname for cname in X_train_full.columns if \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', numerical_transformer,numerical_cols)\n",
    "#    ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.036958,
     "end_time": "2021-01-04T09:46:03.749041",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.712083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039293,
     "end_time": "2021-01-04T09:46:03.826071",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.786778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036315,
     "end_time": "2021-01-04T09:46:03.900808",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.864493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Kandidat Model:\n",
    "- Random Forest\n",
    "- LDA\n",
    "- LGBM\n",
    "- XGBoost\n",
    "- LGBM\n",
    "- Logistic Regression\n",
    "- SVM \n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:03.981167Z",
     "iopub.status.busy": "2021-01-04T09:46:03.980223Z",
     "iopub.status.idle": "2021-01-04T09:46:03.983440Z",
     "shell.execute_reply": "2021-01-04T09:46:03.984075Z"
    },
    "papermill": {
     "duration": 0.046511,
     "end_time": "2021-01-04T09:46:03.984236",
     "exception": false,
     "start_time": "2021-01-04T09:46:03.937725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Define Model\\nfrom sklearn.ensemble import RandomForestRegressor\\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\\n\\nfrom xgboost import XGBRegressor\\nmodel_1 = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\\n\\n# Bundle preprocessing and modeling code in a pipeline\\n#clf = Pipeline(steps=[('preprocessor', preprocessor),\\n#                      ('model', model_1)\\n#                     ])\\n\\n#clf = Pipeline(steps=[('preprocessor', preprocessor)\\n#                     ])\\n\\n# Do Column Transforming without Pipeline\\nX_train_full_trans = preprocessor.fit_transform(X_train_full)\\nX_valid_full_trans = preprocessor.transform(X_valid_full)\\n\\n# Preprocessing of training data, fit model \\n#clf.fit(X_train_full, y_train)\\n\\n#dapet ide pipeline column transform training dulu, trus valid, baru predict\\n\\n#clf.fit(X_train_full, y_train,\\n#            model__early_stopping_rounds=5,\\n#            model__eval_set=[(X_valid_full, y_valid)],\\n#             model__verbose=False)\\nmodel_1.fit(X_train_full_trans, y_train, \\n             early_stopping_rounds=6, \\n             eval_set=[(X_valid_full_trans, y_valid)], \\n             verbose=False)\\n\\n\\n\\n# Preprocessing of validation data, get predictions\\npreds = model_1.predict(X_valid_full_trans)\\ncutoff = 0.5\\npreds = np.where(preds > cutoff, 1, 0)\\n\\n#y_valid_num = y_valid.to_numpy()\\nfrom sklearn import metrics\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import roc_auc_score\\n#confusion_matrix(y_valid, preds)\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "model_1 = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                      ('model', model_1)\n",
    "#                     ])\n",
    "\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor)\n",
    "#                     ])\n",
    "\n",
    "# Do Column Transforming without Pipeline\n",
    "X_train_full_trans = preprocessor.fit_transform(X_train_full)\n",
    "X_valid_full_trans = preprocessor.transform(X_valid_full)\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "#clf.fit(X_train_full, y_train)\n",
    "\n",
    "#dapet ide pipeline column transform training dulu, trus valid, baru predict\n",
    "\n",
    "#clf.fit(X_train_full, y_train,\n",
    "#            model__early_stopping_rounds=5,\n",
    "#            model__eval_set=[(X_valid_full, y_valid)],\n",
    "#             model__verbose=False)\n",
    "model_1.fit(X_train_full_trans, y_train, \n",
    "             early_stopping_rounds=6, \n",
    "             eval_set=[(X_valid_full_trans, y_valid)], \n",
    "             verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = model_1.predict(X_valid_full_trans)\n",
    "cutoff = 0.5\n",
    "preds = np.where(preds > cutoff, 1, 0)\n",
    "\n",
    "#y_valid_num = y_valid.to_numpy()\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#confusion_matrix(y_valid, preds)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036108,
     "end_time": "2021-01-04T09:46:04.057877",
     "exception": false,
     "start_time": "2021-01-04T09:46:04.021769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:04.136296Z",
     "iopub.status.busy": "2021-01-04T09:46:04.135652Z",
     "iopub.status.idle": "2021-01-04T09:46:04.291290Z",
     "shell.execute_reply": "2021-01-04T09:46:04.290143Z"
    },
    "papermill": {
     "duration": 0.196611,
     "end_time": "2021-01-04T09:46:04.291430",
     "exception": false,
     "start_time": "2021-01-04T09:46:04.094819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sys\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:04.378364Z",
     "iopub.status.busy": "2021-01-04T09:46:04.377696Z",
     "iopub.status.idle": "2021-01-04T09:46:06.515029Z",
     "shell.execute_reply": "2021-01-04T09:46:06.516259Z"
    },
    "papermill": {
     "duration": 2.186644,
     "end_time": "2021-01-04T09:46:06.516457",
     "exception": false,
     "start_time": "2021-01-04T09:46:04.329813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 selected features\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection using LGBM\n",
    "# Class_weight to mitigate unbalanced data\n",
    "class_weight = int(y_train.value_counts()[0]/y_train.value_counts()[1])\n",
    "# Hyperparameter tuning\n",
    "lgbc=LGBMClassifier(n_estimators=500, \n",
    "                    learning_rate=0.003, \n",
    "                    sub_feature = 0.5,\n",
    "                    max_bin = 66,\n",
    "                    min_data_in_leaf = 30,\n",
    "                    min_child_samples = 20,\n",
    "                    feature_fraction = 0.5,\n",
    "                    bagging_fraction = 0.8,\n",
    "                    bagging_freq = 40,\n",
    "                    bagging_seed = 11,\n",
    "                    num_leaves=110,\n",
    "                    max_depth = 10,\n",
    "                    colsample_bytree=0.2,\n",
    "                    reg_alpha=1.3, \n",
    "                    reg_lambda=0.1, \n",
    "                    min_split_gain=0.01, \n",
    "                    min_child_weight=40,\n",
    "                    metric = 'auc',\n",
    "                    is_unbalance = True)\n",
    "                   #scale_post_weight = class_weight)\n",
    "\n",
    "# Feature Selection\n",
    "embeded_lgb_selector = SelectFromModel(lgbc, max_features=27)\n",
    "embeded_lgb_selector.fit(X_train_full, y_train)\n",
    "\n",
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = X_train_full.loc[:,embeded_lgb_support].columns.tolist()\n",
    "print(str(len(embeded_lgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:06.626403Z",
     "iopub.status.busy": "2021-01-04T09:46:06.625620Z",
     "iopub.status.idle": "2021-01-04T09:46:06.629494Z",
     "shell.execute_reply": "2021-01-04T09:46:06.630080Z"
    },
    "papermill": {
     "duration": 0.062698,
     "end_time": "2021-01-04T09:46:06.630269",
     "exception": false,
     "start_time": "2021-01-04T09:46:06.567571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping the columns that's less important\n",
    "drop_feat = [cols for cols in all_cols if not(cols in embeded_lgb_feature)]\n",
    "#X_train_full = X_train_full.drop(drop_feat, axis=1)\n",
    "#X_valid_full = X_valid_full.drop(drop_feat, axis=1)\n",
    "X_test_full = X_test_full.drop(drop_feat, axis = 1)\n",
    "#X_CV =  X_CV.drop(drop_feat, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:06.727719Z",
     "iopub.status.busy": "2021-01-04T09:46:06.727035Z",
     "iopub.status.idle": "2021-01-04T09:46:08.346612Z",
     "shell.execute_reply": "2021-01-04T09:46:08.347745Z"
    },
    "papermill": {
     "duration": 1.668278,
     "end_time": "2021-01-04T09:46:08.347932",
     "exception": false,
     "start_time": "2021-01-04T09:46:06.679654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5605958029577071\n"
     ]
    }
   ],
   "source": [
    "lgbc.fit(X_train_full, y_train)\n",
    "y_pred = lgbc.predict_proba(X_valid_full)\n",
    "#y_pred = lgbc.predict(X_valid_full)\n",
    "#np.unique(y_pred)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "print(roc_auc_score(y_valid, y_pred[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064079,
     "end_time": "2021-01-04T09:46:08.463928",
     "exception": false,
     "start_time": "2021-01-04T09:46:08.399849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:08.587336Z",
     "iopub.status.busy": "2021-01-04T09:46:08.586330Z",
     "iopub.status.idle": "2021-01-04T09:46:16.248583Z",
     "shell.execute_reply": "2021-01-04T09:46:16.249209Z"
    },
    "papermill": {
     "duration": 7.717383,
     "end_time": "2021-01-04T09:46:16.249373",
     "exception": false,
     "start_time": "2021-01-04T09:46:08.531990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:46:09] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { bagging_fraction, bagging_freq, bagging_seed, feature_fraction, metric, min_child_samples, min_data_in_leaf, min_split_gain, num_leaves, sub_feature } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.5740714913185516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#lgbc.fit(X_train_full, y_train)\n",
    "#y_pred = lgbc.predict(X_valid_full)\n",
    "#print(np.unique(y_pred))\n",
    "#X_valid_full\n",
    "from xgboost import XGBClassifier\n",
    "#model_1 = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "model_1 = XGBClassifier(n_estimators=500, \n",
    "                    learning_rate=0.003, \n",
    "                    sub_feature = 0.5,\n",
    "                    max_bin = 66,\n",
    "                    min_data_in_leaf = 30,\n",
    "                    min_child_samples = 20,\n",
    "                    feature_fraction = 0.5,\n",
    "                    bagging_fraction = 0.8,\n",
    "                    bagging_freq = 40,\n",
    "                    bagging_seed = 11,\n",
    "                    num_leaves=110,\n",
    "                    max_depth = 10,\n",
    "                    colsample_bytree=0.2,\n",
    "                    reg_alpha=1.3, \n",
    "                    reg_lambda=0.1, \n",
    "                    min_split_gain=0.01, \n",
    "                    min_child_weight=40,\n",
    "                    objective ='binary:logistic',\n",
    "                    metric = 'auc',\n",
    "                    scale_pos_weight = class_weight)\n",
    "model_1.fit(X_train_full, y_train)\n",
    "y_pred = model_1.predict_proba(X_valid_full)\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "print(roc_auc_score(y_valid, y_pred[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049915,
     "end_time": "2021-01-04T09:46:16.349590",
     "exception": false,
     "start_time": "2021-01-04T09:46:16.299675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:16.454902Z",
     "iopub.status.busy": "2021-01-04T09:46:16.454233Z",
     "iopub.status.idle": "2021-01-04T09:46:47.238744Z",
     "shell.execute_reply": "2021-01-04T09:46:47.238201Z"
    },
    "papermill": {
     "duration": 30.839025,
     "end_time": "2021-01-04T09:46:47.238852",
     "exception": false,
     "start_time": "2021-01-04T09:46:16.399827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC AUC: 0.59602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import cross_val_score\\n\\n# Multiply by -1 since sklearn calculates *negative* MAE\\nscores = -1 * cross_val_score(clf, X_full, y,\\n                              cv=5,\\n                              scoring=\\'roc_auc\\')\\n\\nprint(\"AUC scores:\\n\", scores)\\n# Cross Validate scorenya kok jauh ?\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit balanced xgboost on an imbalanced classification dataset\n",
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "# define evaluation procedure\n",
    "#cv = RepeatedStratifiedKFold(n_splits=20, n_repeats=1, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model_1, X_CV, y_CV, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.5f' % mean(scores))\n",
    "'''\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(clf, X_full, y,\n",
    "                              cv=5,\n",
    "                              scoring='roc_auc')\n",
    "\n",
    "print(\"AUC scores:\\n\", scores)\n",
    "# Cross Validate scorenya kok jauh ?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:47.325295Z",
     "iopub.status.busy": "2021-01-04T09:46:47.324262Z",
     "iopub.status.idle": "2021-01-04T09:46:47.327627Z",
     "shell.execute_reply": "2021-01-04T09:46:47.328615Z"
    },
    "papermill": {
     "duration": 0.049564,
     "end_time": "2021-01-04T09:46:47.328797",
     "exception": false,
     "start_time": "2021-01-04T09:46:47.279233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5947391984261103\n",
      "0.5560254340710618\n",
      "0.5502508985567884\n",
      "0.6357920035951956\n",
      "0.6432886936325715\n"
     ]
    }
   ],
   "source": [
    "for score in scores:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040424,
     "end_time": "2021-01-04T09:46:47.409773",
     "exception": false,
     "start_time": "2021-01-04T09:46:47.369349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-04T09:46:47.499300Z",
     "iopub.status.busy": "2021-01-04T09:46:47.498641Z",
     "iopub.status.idle": "2021-01-04T09:46:47.635747Z",
     "shell.execute_reply": "2021-01-04T09:46:47.635124Z"
    },
    "papermill": {
     "duration": 0.185518,
     "end_time": "2021-01-04T09:46:47.635871",
     "exception": false,
     "start_time": "2021-01-04T09:46:47.450353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields age, year_graduated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8b30751a83a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediksi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprediksi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediksi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcoba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Best Performance'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mprediksi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcoba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcoba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, data, ntree_limit, validate_features, base_margin)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \"\"\"\n\u001b[1;32m    942\u001b[0m         test_dmatrix = DMatrix(data, base_margin=base_margin,\n\u001b[0;32m--> 943\u001b[0;31m                                missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_ntree_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             feature_types=feature_types)\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         return _from_pandas_df(data, missing, threads,\n\u001b[0;32m--> 504\u001b[0;31m                                feature_names, feature_types)\n\u001b[0m\u001b[1;32m    505\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         return _from_pandas_series(data, missing, threads, feature_names,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_pandas_df\u001b[0;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     data, feature_names, feature_types = _transform_pandas_df(\n\u001b[0;32m--> 219\u001b[0;31m         data, feature_names, feature_types)\n\u001b[0m\u001b[1;32m    220\u001b[0m     return _from_numpy_array(data, missing, nthread, feature_names,\n\u001b[1;32m    221\u001b[0m                              feature_types)\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, feature_names, feature_types, meta, meta_type)\u001b[0m\n\u001b[1;32m    183\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    184\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields age, year_graduated"
     ]
    }
   ],
   "source": [
    "prediksi = model_1.predict_proba(X_test_full)\n",
    "prediksi = pd.DataFrame(prediksi)\n",
    "coba = pd.DataFrame({'index': [i for i in range(6000)],'Best Performance' :prediksi[1]})\n",
    "coba.index = coba['index']\n",
    "coba = coba.drop(columns = ['index'])\n",
    "coba.to_csv('xgb_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040906,
     "end_time": "2021-01-04T09:46:47.718118",
     "exception": false,
     "start_time": "2021-01-04T09:46:47.677212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 124.797221,
   "end_time": "2021-01-04T09:46:47.868179",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-04T09:44:43.070958",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
